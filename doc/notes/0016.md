For automated testing, I'm going to use pytest, and the bash shell
as my mechanism for finding which tests to run.

Choice of framework for developer tests is another good excuse to
add a decision record to the repository.

I normally begin with a calibration test, to ensure that the test
framework is running my tests, and that I understand how failing
tests are reported.

I wouldn't normally check in a failing test, but in the context of
a running narrative I think it helps those trying to follow along
if I show intermediate steps.

It will add a bit to the punishment if I need to bisect my way
to the cause of a mysterious error later.

